{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.3.0-py2.py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyOpenSSL>=16.2.0\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from scrapy) (5.1.0)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from scrapy) (4.5.2)\n",
      "Collecting cryptography>=2.0\n",
      "  Downloading cryptography-3.1-cp35-abi3-manylinux2010_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 3.4 MB/s eta 0:00:01     |███████████████████████████     | 2.2 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.3-py3-none-any.whl (11 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.1.0-py3-none-any.whl (7.0 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-20.3.0-cp37-cp37m-manylinux1_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from pyOpenSSL>=16.2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (47.3.1)\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Using cached cffi-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (401 kB)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /home/iazzari/.local/lib/python3.7/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: pyasn1 in /home/iazzari/.local/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 579 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-20.0.1-py2.py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 4.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Using cached pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: idna>=2.5 in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.9)\n",
      "Building wheels for collected packages: PyDispatcher, protego\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=3a0ccb5e3e276160aa9fc3897db1244bfa2001941ce7bceb0338f1debfef60f1\n",
      "  Stored in directory: /home/iazzari/.cache/pip/wheels/dc/d0/bf/0cc715c01fce0bace63b46283acf5cc630d5e5dbb4602c54e5\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=6b3e154a12c98e6f73f106eadd62ef97b0d2805a4bf8c0de7760081315fc71e2\n",
      "  Stored in directory: /home/iazzari/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "Successfully built PyDispatcher protego\n",
      "Installing collected packages: pycparser, cffi, cryptography, pyOpenSSL, PyDispatcher, w3lib, itemadapter, cssselect, parsel, itemloaders, queuelib, protego, service-identity, incremental, Automat, PyHamcrest, constantly, hyperlink, Twisted, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 cffi-1.14.2 constantly-15.1.0 cryptography-3.1 cssselect-1.1.0 hyperlink-20.0.1 incremental-17.5.0 itemadapter-0.1.0 itemloaders-1.0.3 parsel-1.6.0 protego-0.1.16 pyOpenSSL-19.1.0 pycparser-2.20 queuelib-1.5.0 scrapy-2.3.0 service-identity-18.1.0 w3lib-1.22.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/iazzari/miniconda3/envs/investment_scripts/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for time\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/iazzari/miniconda3/envs/investment_scripts/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: datetime in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (4.3)\n",
      "Requirement already satisfied: pytz in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from datetime) (2020.1)\n",
      "Requirement already satisfied: zope.interface in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from datetime) (5.1.0)\n",
      "Requirement already satisfied: setuptools in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from zope.interface->datetime) (47.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/iazzari/miniconda3/envs/investment_scripts/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (1.0.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/iazzari/.local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from pandas) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/iazzari/miniconda3/envs/investment_scripts/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/iazzari/miniconda3/envs/investment_scripts/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Scrapy\n",
    "!pip install time\n",
    "!pip install datetime\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GET https://br.investing.com/search/?q=BRML3&tab=quotes>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapy.Request(url='https://br.investing.com/search/?q=BRML3&tab=quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://br.investing.com/')\n",
    "response = TextResponse(res.url, body=res.text, encoding='utf-8')\n",
    "\n",
    "# notice = \n",
    "response.xpath('/html/body/div[5]/section/div[1]/div[1]/div[1]/div[1]/article/div[1]/a').extract_first()\n",
    "\n",
    "\n",
    "# print(notice.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2ce73ab24940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(response.xpath(\"/html/body/div[6]/div/div/div[2]/div[1]/div[3]/div[1]/a/h2\").extract_first().text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"article\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div.texts h2 a::attr(href)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(response.xpath(\"/html/body/div[5]/section/div/div[3]/div[3]/div\").extract_first())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://tecnoblog.net/')\n",
    "\n",
    "# print(response.xpath(\"/html/body/div[6]/div/div/div[2]/div[1]/div[3]/div[1]/a/h2\").extract_first().text)\n",
    "\n",
    "for i in response.css(\"article\").extract_first():\n",
    "    print(i.css(\"div.texts h2 a::attr(href)\").extract_first())\n",
    "# print(response.xpath(\"/html/body/div[5]/section/div/div[3]/div[3]/div\").extract_first())\n",
    "\n",
    "# response.xpath(\"//item/title\").extract_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-13 12:23:53 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-13 12:23:53 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.7 (default, May  7 2020, 21:25:33) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.1, Platform Linux-5.4.0-42-generic-x86_64-with-debian-buster-sid\n",
      "2020-09-13 12:23:53 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Spider not found: your-spider'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/investment_scripts/lib/python3.7/site-packages/scrapy/spiderloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, spider_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spiders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspider_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'your-spider'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d7881b2e651b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_project_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your-spider'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the script will block here until the crawling is finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/investment_scripts/lib/python3.7/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self, crawler_or_spidercls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;34m'The crawler_or_spidercls argument cannot be a spider object, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 'it must be a spider class (or a Crawler object)')\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mcrawler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/investment_scripts/lib/python3.7/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mcreate_crawler\u001b[0;34m(self, crawler_or_spidercls)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrawler_or_spidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/investment_scripts/lib/python3.7/site-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36m_create_crawler\u001b[0;34m(self, spidercls)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspidercls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mspidercls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspider_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspidercls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspidercls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/investment_scripts/lib/python3.7/site-packages/scrapy/spiderloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, spider_name)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spiders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspider_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spider not found: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspider_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_by_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Spider not found: your-spider'"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "process.crawl('your-spider')\n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-13 12:28:48 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-13 12:28:48 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.7 (default, May  7 2020, 21:25:33) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.1, Platform Linux-5.4.0-42-generic-x86_64-with-debian-buster-sid\n",
      "2020-09-13 12:28:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-09-13 12:28:48 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2020-09-13 12:28:48 [scrapy.extensions.telnet] INFO: Telnet Password: 4ee000aa933ad962\n",
      "2020-09-13 12:28:48 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    # Your spider definition\n",
    "    ...\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    \"FEEDS\": {\n",
    "        \"items.json\": {\"format\": \"json\"},\n",
    "    },\n",
    "})\n",
    "\n",
    "process.crawl(MySpider)\n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for title in response.css('.post-header>h2'):\n",
    "            yield {'title': title.css('a ::text').get()}\n",
    "\n",
    "        for next_page in response.css('a.next-posts-link'):\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
